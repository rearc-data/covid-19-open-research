---
AWSTemplateFormatVersion: '2010-09-09'
Transform: "AWS::Serverless-2016-10-31"
Description: "CloudFormation template to get automated revision downloads from AWS Data Exchange upon publish Cloudwatch event. Additionally, to create a database in AWS Glue and running a SQL query to create a new table in Athena using AWS Lambda"

Parameters:
  SourceS3Bucket:
    Type: String
    Description: Provide the S3 Bucket name where this dataset is going to be stored
  Region:
    Type: String
    Default: us-east-1
    Description: Region for the subscribed AWS Data Exchange dataset
  DataSetName:
    Type: String
    Default: co2-emissions
    Description: Name of the AWS Data Exchange dataset
  DataSetArn:
    Type: String
    Default: arn:aws:dataexchange:us-east-1::DataSet/925edeff61680b5dd07bfe846051a6c7
    Description: Provide the Data Set ARN for the subscribed AWS Data Exchange data set

Mappings:
  RegionMap:
    "eu-north-1":
       AppId: "arn:aws:serverlessrepo:eu-north-1:697637923817:applications/dataexchangesdk"
    "ap-south-1":
       AppId: "arn:aws:serverlessrepo:ap-south-1:697637923817:applications/dataexchangesdk"
    "eu-west-3":
       AppId: "arn:aws:serverlessrepo:eu-west-3:697637923817:applications/dataexchangesdk"
    "eu-west-2":
       AppId: "arn:aws:serverlessrepo:eu-west-2:697637923817:applications/dataexchangesdk"
    "eu-west-1":
       AppId: "arn:aws:serverlessrepo:eu-west-1:697637923817:applications/dataexchangesdk"
    "ap-northeast-3":
       AppId: "arn:aws:serverlessrepo:ap-northeast-3:697637923817:applications/dataexchangesdk"
    "ap-northeast-2":
       AppId: "arn:aws:serverlessrepo:ap-northeast-2:697637923817:applications/dataexchangesdk"
    "ap-northeast-1":
       AppId: "arn:aws:serverlessrepo:ap-northeast-1:697637923817:applications/dataexchangesdk"
    "sa-east-1":
       AppId: "arn:aws:serverlessrepo:sa-east-1:697637923817:applications/dataexchangesdk"
    "ca-central-1":
       AppId: "arn:aws:serverlessrepo:ca-central-1:697637923817:applications/dataexchangesdk"
    "ap-southeast-1":
       AppId: "arn:aws:serverlessrepo:ap-southeast-1:697637923817:applications/dataexchangesdk"
    "ap-southeast-2":
       AppId: "arn:aws:serverlessrepo:ap-southeast-2:697637923817:applications/dataexchangesdk"
    "eu-central-1":
       AppId: "arn:aws:serverlessrepo:eu-central-1:697637923817:applications/dataexchangesdk"
    "us-east-1":
       AppId: "arn:aws:serverlessrepo:us-east-1:697637923817:applications/dataexchangesdk"
    "us-east-2":
       AppId: "arn:aws:serverlessrepo:us-east-2:697637923817:applications/dataexchangesdk"
    "us-west-1":
       AppId: "arn:aws:serverlessrepo:us-west-1:697637923817:applications/dataexchangesdk"
    "us-west-2":
       AppId: "arn:aws:serverlessrepo:us-west-2:697637923817:applications/dataexchangesdk"

Resources:
  RevisionLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action:
            - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
              - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
      Path: /
      Policies:
        -
          PolicyName:
            !Join
              - ''
              - - 'dataexchange-revision-for-'
                - !Ref DataSetName
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              -
                Effect: "Allow"
                Action:
                  - "dataexchange:StartJob"
                  - "dataexchange:CreateJob"
                  - "dataexchange:GetJob"
                  - "dataexchange:ListRevisionAssets"
                  - "dataexchange:ListDataSetRevisions"
                Resource: "*"
        -
          PolicyName:
            !Join
              - ''
              - - 's3-revision-for-'
                - !Ref DataSetName
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              -
                Effect: "Allow"
                Action: "s3:PutObject"
                Resource: !Sub "arn:aws:s3:::${SourceS3Bucket}/*"
        -
          PolicyName:
            !Join
              - ''
              - - 'dataexchange-s3-revision-for-'
                - !Ref DataSetName
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              -
                Effect: "Allow"
                Action: "s3:GetObject"
                Resource:
                  - "arn:aws:s3:::*aws-data-exchange*"

  dataexchangesdk:
    Type: AWS::Serverless::Application
    Properties:
      Location:
        ApplicationId: !FindInMap [RegionMap, !Ref "AWS::Region", AppId]
        SemanticVersion: 0.0.3

# Create a Lambda function that will update revisions to dataexchange dataset
  RevisionLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Layers:
        - !GetAtt dataexchangesdk.Outputs.LayerArn
      FunctionName:
        !Join
          - ''
          - - 'revision-updates-for-'
            - !Ref DataSetName
      Description: "Revision updates to AWS Data Exchange data set"
      Runtime: "python3.6"
      Code:
        S3Bucket:
          Fn::Sub: ${SourceS3Bucket}
        S3Key:
          !Join
            - ''
            - - Fn::Sub: ${DataSetName}
              - '/automation/post-processing-code.zip'
      Handler: "lambda_function.lambda_handler"
      MemorySize: 128
      ReservedConcurrentExecutions: 1
      Timeout: 120
      Role:
        Fn::GetAtt:
          - RevisionLambdaRole
          - Arn
      Environment:
        Variables:
          REGION:
            Fn::Sub: ${Region}
          S3_BUCKET:
            Fn::Sub: ${SourceS3Bucket}
          DATA_SET_ARN:
            Fn::Sub: ${DataSetArn}

  EventRule:
    Type: AWS::Events::Rule
    Properties:
      Description: "Revision event rule"
      EventPattern:
        source:
          - "aws.dataexchange"
        detail-type:
          - "Revision Published To Data Set"
        resources:
          - Fn::Sub: ${DataSetArn}
      State: "ENABLED"
      Targets:
        -
          Arn:
            Fn::GetAtt:
              - "RevisionLambdaFunction"
              - "Arn"
          Id: "TargetFunctionV1"

  PermissionForEventsToInvokeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName:
        Ref: "RevisionLambdaFunction"
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn:
        Fn::GetAtt:
          - "EventRule"
          - "Arn"

  InvokeRevisionLambda:
    Type: Custom::InvokeLambda
    DependsOn: RevisionLambdaFunction
    Properties:
      ServiceToken: !GetAtt RevisionLambdaFunction.Arn

# Create an AWS Glue database
  Database:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: "co2_emissions"
        Description: "This database holds schema information for your dataset"

# Create an IAM role for Lambda to use
  AnalyticsLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Statement:
          - Action:
            - sts:AssumeRole
            Effect: Allow
            Principal:
              Service:
              - lambda.amazonaws.com
        Version: 2012-10-17
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/AWSLambdaExecute
        - arn:aws:iam::aws:policy/AmazonAthenaFullAccess
      Path: /

# Create a Lambda function that will run an Athena query to create a new table with the schema
  AnalyticsLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName:
        !Join
          - ''
          - - 'create-table-'
            - !Ref DataSetName
      Description: "A query that creates the table with appropriate schema"
      Runtime: "python2.7"
      Code:
        ZipFile: |
          import os
          import boto3
          from botocore.vendored import requests
          import json

          DATABASE = 'co2_emissions'
          TABLE = 'data'
          SourceS3BucketPath = 's3://' + os.environ['SOURCE_S3_BUCKET'] + '/' + os.environ['DATA_SET_NAME'] + '/dataset/'
          OUTPUT = SourceS3BucketPath + 'output'

          def lambda_handler(event, context):
            try:
              query = \
              """CREATE EXTERNAL TABLE %s.%s (
              `country_name` string,
              `country_code` string,
              `indicator_name` string,
              `indicator_code` string,
              `1960` decimal(19,3),
              `1961` decimal(19,3),
              `1962` decimal(19,3),
              `1963` decimal(19,3),
              `1964` decimal(19,3),
              `1965` decimal(19,3),
              `1966` decimal(19,3),
              `1967` decimal(19,3),
              `1968` decimal(19,3),
              `1969` decimal(19,3),
              `1970` decimal(19,3),
              `1971` decimal(19,3),
              `1972` decimal(19,3),
              `1973` decimal(19,3),
              `1974` decimal(19,3),
              `1975` decimal(19,3),
              `1976` decimal(19,3),
              `1977` decimal(19,3),
              `1978` decimal(19,3),
              `1979` decimal(19,3),
              `1980` decimal(19,3),
              `1981` decimal(19,3),
              `1982` decimal(19,3),
              `1983` decimal(19,3),
              `1984` decimal(19,3),
              `1985` decimal(19,3),
              `1986` decimal(19,3),
              `1987` decimal(19,3),
              `1988` decimal(19,3),
              `1989` decimal(19,3),
              `1990` decimal(19,3),
              `1991` decimal(19,3),
              `1992` decimal(19,3),
              `1993` decimal(19,3),
              `1994` decimal(19,3),
              `1995` decimal(19,3),
              `1996` decimal(19,3),
              `1997` decimal(19,3),
              `1998` decimal(19,3),
              `1999` decimal(19,3),
              `2000` decimal(19,3),
              `2001` decimal(19,3),
              `2002` decimal(19,3),
              `2003` decimal(19,3),
              `2004` decimal(19,3),
              `2005` decimal(19,3),
              `2006` decimal(19,3),
              `2007` decimal(19,3),
              `2008` decimal(19,3),
              `2009` decimal(19,3),
              `2010` decimal(19,3),
              `2011` decimal(19,3),
              `2012` decimal(19,3),
              `2013` decimal(19,3),
              `2014` decimal(19,3),
              `2015` decimal(19,3),
              `2016` decimal(19,3),
              `2017` decimal(19,3),
              `2018` decimal(19,3),
              `2019` decimal(19,3)
              )
              ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
              WITH SERDEPROPERTIES (
              'serialization.format' = ',', 'field.delim' = ','
              ) LOCATION '%s'
              TBLPROPERTIES (
              'has_encrypted_data'='false',
              'skip.header.line.count'='1');""" % ( DATABASE, TABLE, SourceS3BucketPath )

              client = boto3.client('athena')

              # Execution
              response = client.start_query_execution(
                QueryString=query,
                QueryExecutionContext={
                  'Database': DATABASE
                },
                ResultConfiguration={
                  'OutputLocation': OUTPUT,
                }
              )

              sendResponse(event, context, 'SUCCESS', {'Status': 'SUCCESS'})

            except Exception as e:
              print(str(e))
              sendResponse(event, context, 'FAILED', {'Status': 'FAILED'})

          def sendResponse(event, context, responseStatus, responseData):
            responseBody = {
              'Status': responseStatus,
              'Reason': 'See the details in CloudWatch Log Stream: ' + context.log_stream_name,
              'PhysicalResourceId': context.log_stream_name,
              'StackId': event['StackId'],
              'RequestId': event['RequestId'],
              'LogicalResourceId': event['LogicalResourceId'],
              'Data': responseData
            }
            json_responseBody = json.dumps(responseBody)
            headers = {
              'content-type' : '',
              'content-length' : str(len(json_responseBody))
            }
            try:
              response = requests.put(
                event['ResponseURL'],
                data=json_responseBody,
                headers=headers
              )
              print("Status code: " + response.reason)
            except Exception as e:
              print("send(..) failed executing requests.put(..): " + str(e))
      Handler: "index.lambda_handler"
      MemorySize: 128
      Timeout: 10
      Role:
        Fn::GetAtt:
          - AnalyticsLambdaRole
          - Arn
      Environment:
        Variables:
          SOURCE_S3_BUCKET:
            Fn::Sub: ${SourceS3Bucket}
          DATA_SET_NAME:
            Fn::Sub: ${DataSetName}

  InvokeAnalyticsLambda:
    Type: Custom::InvokeLambda
    DependsOn:
      - InvokeRevisionLambda
      - AnalyticsLambdaFunction
    Properties:
      ServiceToken: !GetAtt AnalyticsLambdaFunction.Arn

Outputs:
  AnalyticsLambdaRoleARN:
    Description: Role for Analytics Lambda execution.
    Value:
      Fn::GetAtt:
        - AnalyticsLambdaRole
        - Arn
  AnalyticsLambdaFunctionName:
    Value:
      Ref: AnalyticsLambdaFunction
  RevisionLambdaRoleARN:
    Description: Role for Revision Lambda execution.
    Value:
      Fn::GetAtt:
        - RevisionLambdaRole
        - Arn
  RevisionLambdaFunctionName:
    Value:
      Ref: RevisionLambdaFunction
